{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2c6041ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "26aef802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ef7ca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID column\n",
    "data = df.drop(columns=['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "16555479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input Columns: SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm\n",
    "Target Columns: SetosaScore, VersicolorScore, VirginicaScore\n",
    "\"\"\"\n",
    "x = data.drop(columns=['SetosaScore', 'VersicolorScore', 'VirginicaScore']).values\n",
    "y = df.loc[0:, ['SetosaScore', 'VersicolorScore', 'VirginicaScore']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "81a30a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch scikit-learn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b49c1acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAdditional guidelines are as follows:\\n\\nEnsure the training loss and the test loss are less than 0.27. \\nUse only Linear layers in the neural network.\\nDo not use any activation functions like ReLU or others\\nDo not use any regularization\\nUse Adam optimizer\\nUse MAE (Mean Absolute Loss)\\nSplit the dataset to train-test with a ratio of 80% for training and 20% for testing.\\n'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Additional guidelines are as follows:\n",
    "\n",
    "Ensure the training loss and the test loss are less than 0.27. \n",
    "Use only Linear layers in the neural network.\n",
    "Do not use any activation functions like ReLU or others\n",
    "Do not use any regularization\n",
    "Use Adam optimizer\n",
    "Use MAE (Mean Absolute Loss)\n",
    "Split the dataset to train-test with a ratio of 80% for training and 20% for testing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "46d684cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (80/20 split)]\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "#  Specifies the proportion of the dataset that \n",
    "# should be allocated to the test set. \n",
    "# Here, 0.2 means 20% of the data will be used\n",
    "# for testing, and the remaining 80% will be \n",
    "# used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ff79d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# StandardScaler: This is a class from \n",
    "# Scikit-learn used to standardize \n",
    "# features by removing the mean and \n",
    "# scaling to unit variance.\n",
    "# Standardization: This process involves\n",
    "# rescaling the features so that they have \n",
    "# a mean of 0 and a standard deviation of 1.\n",
    "# This is important for many machine learning\n",
    "# algorithms that perform better when \n",
    "# features are on a similar scale.\n",
    "\n",
    "# This process is crucial for ensuring \n",
    "# that the model performs consistently,\n",
    "# as many machine learning algorithms \n",
    "# assume or perform better when the \n",
    "# input features are on a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "38de0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch sensors\n",
    "x_train_tensor = torch.tensor(\n",
    "    x_train, dtype=torch.float32\n",
    ")\n",
    "# torch.tensor(X_train): Converts \n",
    "# the X_train data (which is likely \n",
    "# a NumPy array or a Pandas \n",
    "# DataFrame) into a PyTorch tensor.\n",
    "# dtype=torch.float32: Specifies that \n",
    "# the data type of the tensor should \n",
    "# be float32, which is a common \n",
    "# choice for numerical data in \n",
    "# deep learning models.\n",
    "# Result: X_train_tensor is now a \n",
    "# PyTorch tensor containing the \n",
    "# standardized training input data, \n",
    "# ready for use in a neural network.\n",
    "\n",
    "y_train_tensor = torch.tensor(\n",
    "    y_train, dtype=torch.float32)\n",
    "\n",
    "x_test_tensor = torch.tensor(\n",
    "    x_test, dtype=torch.float32\n",
    ")\n",
    "\n",
    "y_test_tensor = torch.tensor(\n",
    "    y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "95f20d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(\n",
    "    x_train_tensor, y_train_tensor\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=15, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "204bae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "#  defines a new class NeuralNetwork \n",
    "# that inherits from nn.Module. \n",
    "# By inheriting from nn.Module, \n",
    "# the NeuralNetwork class gains access \n",
    "# to all the functionalities provided \n",
    "# by PyTorch for building and managing neural networks.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # Start by calling the constructor of \n",
    "    # the parent class (nn.Module), ensuring \n",
    "    # that the class is properly initialized.\n",
    "    def __init__(self) -> None: # Constructor\n",
    "        super(NeuralNetwork, self).__init__() \n",
    "        # nn.Linear creates a fully connected (linear) layer.\n",
    "        self.fc1 = nn.Linear(x_train.shape[1], 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "        self.fc4 = nn.Linear(512, 3)\n",
    "\n",
    "    # The forward method defines the forward pass of \n",
    "    # the network. It specifies how the input tensor\n",
    "    # x should flow through the layers of the network.\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) # No activation after the first layer\n",
    "        x = self.fc2(x) # No activation afer the second layer\n",
    "        x = self.fc3(x) # No activation afer the second layer\n",
    "        x = self.fc4(x) # Output layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.L1Loss() # for mean absolute loss\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr= 0.001 #learning rate of 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2657be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 0.2656\n",
      "Epoch: 2, Average Loss: 0.2657\n",
      "Epoch: 3, Average Loss: 0.2655\n",
      "Epoch: 4, Average Loss: 0.2651\n",
      "Epoch: 5, Average Loss: 0.2656\n",
      "Epoch: 6, Average Loss: 0.2653\n",
      "Epoch: 7, Average Loss: 0.2653\n",
      "Epoch: 8, Average Loss: 0.2652\n",
      "Epoch: 9, Average Loss: 0.2649\n",
      "Epoch: 10, Average Loss: 0.2637\n",
      "Epoch: 11, Average Loss: 0.2642\n",
      "Epoch: 12, Average Loss: 0.2647\n",
      "Epoch: 13, Average Loss: 0.2647\n",
      "Epoch: 14, Average Loss: 0.2645\n",
      "Epoch: 15, Average Loss: 0.2646\n",
      "Epoch: 16, Average Loss: 0.2637\n",
      "Epoch: 17, Average Loss: 0.2649\n",
      "Epoch: 18, Average Loss: 0.2644\n",
      "Epoch: 19, Average Loss: 0.2648\n",
      "Epoch: 20, Average Loss: 0.2648\n",
      "Epoch: 21, Average Loss: 0.2639\n",
      "Epoch: 22, Average Loss: 0.2644\n",
      "Epoch: 23, Average Loss: 0.2650\n",
      "Epoch: 24, Average Loss: 0.2648\n",
      "Epoch: 25, Average Loss: 0.2647\n",
      "Epoch: 26, Average Loss: 0.2649\n",
      "Epoch: 27, Average Loss: 0.2642\n",
      "Epoch: 28, Average Loss: 0.2642\n",
      "Epoch: 29, Average Loss: 0.2651\n",
      "Epoch: 30, Average Loss: 0.2649\n",
      "Epoch: 31, Average Loss: 0.2648\n",
      "Epoch: 32, Average Loss: 0.2642\n",
      "Epoch: 33, Average Loss: 0.2648\n",
      "Epoch: 34, Average Loss: 0.2647\n",
      "Epoch: 35, Average Loss: 0.2656\n",
      "Epoch: 36, Average Loss: 0.2651\n",
      "Epoch: 37, Average Loss: 0.2647\n",
      "Epoch: 38, Average Loss: 0.2652\n",
      "Epoch: 39, Average Loss: 0.2648\n",
      "Epoch: 40, Average Loss: 0.2649\n",
      "Epoch: 41, Average Loss: 0.2653\n",
      "Epoch: 42, Average Loss: 0.2655\n",
      "Epoch: 43, Average Loss: 0.2663\n",
      "Epoch: 44, Average Loss: 0.2650\n",
      "Epoch: 45, Average Loss: 0.2653\n",
      "Epoch: 46, Average Loss: 0.2654\n",
      "Epoch: 47, Average Loss: 0.2653\n",
      "Epoch: 48, Average Loss: 0.2655\n",
      "Epoch: 49, Average Loss: 0.2656\n",
      "Epoch: 50, Average Loss: 0.2654\n",
      "Epoch: 51, Average Loss: 0.2659\n",
      "Epoch: 52, Average Loss: 0.2656\n",
      "Epoch: 53, Average Loss: 0.2676\n",
      "Epoch: 54, Average Loss: 0.2657\n",
      "Epoch: 55, Average Loss: 0.2680\n",
      "Epoch: 56, Average Loss: 0.2782\n",
      "Epoch: 57, Average Loss: 0.2684\n",
      "Epoch: 58, Average Loss: 0.2726\n",
      "Epoch: 59, Average Loss: 0.2834\n",
      "Epoch: 60, Average Loss: 0.2788\n",
      "Epoch: 61, Average Loss: 0.2719\n",
      "Epoch: 62, Average Loss: 0.2764\n",
      "Epoch: 63, Average Loss: 0.2747\n",
      "Epoch: 64, Average Loss: 0.2780\n",
      "Epoch: 65, Average Loss: 0.2739\n",
      "Epoch: 66, Average Loss: 0.2763\n",
      "Epoch: 67, Average Loss: 0.2681\n",
      "Epoch: 68, Average Loss: 0.2689\n",
      "Epoch: 69, Average Loss: 0.2828\n",
      "Epoch: 70, Average Loss: 0.2729\n",
      "Epoch: 71, Average Loss: 0.2739\n",
      "Epoch: 72, Average Loss: 0.2816\n",
      "Epoch: 73, Average Loss: 0.2721\n",
      "Epoch: 74, Average Loss: 0.2720\n",
      "Epoch: 75, Average Loss: 0.2725\n",
      "Epoch: 76, Average Loss: 0.2687\n",
      "Epoch: 77, Average Loss: 0.2763\n",
      "Epoch: 78, Average Loss: 0.2671\n",
      "Epoch: 79, Average Loss: 0.2687\n",
      "Epoch: 80, Average Loss: 0.2664\n",
      "Epoch: 81, Average Loss: 0.2654\n",
      "Epoch: 82, Average Loss: 0.2645\n",
      "Epoch: 83, Average Loss: 0.2657\n",
      "Epoch: 84, Average Loss: 0.2656\n",
      "Epoch: 85, Average Loss: 0.2653\n",
      "Epoch: 86, Average Loss: 0.2643\n",
      "Epoch: 87, Average Loss: 0.2651\n",
      "Epoch: 88, Average Loss: 0.2644\n",
      "Epoch: 89, Average Loss: 0.2650\n",
      "Epoch: 90, Average Loss: 0.2656\n",
      "Epoch: 91, Average Loss: 0.2649\n",
      "Epoch: 92, Average Loss: 0.2650\n",
      "Epoch: 93, Average Loss: 0.2644\n",
      "Epoch: 94, Average Loss: 0.2645\n",
      "Epoch: 95, Average Loss: 0.2647\n",
      "Epoch: 96, Average Loss: 0.2657\n",
      "Epoch: 97, Average Loss: 0.2641\n",
      "Epoch: 98, Average Loss: 0.2658\n",
      "Epoch: 99, Average Loss: 0.2666\n",
      "Epoch: 100, Average Loss: 0.2653\n",
      "Epoch: 101, Average Loss: 0.2653\n",
      "Epoch: 102, Average Loss: 0.2649\n",
      "Epoch: 103, Average Loss: 0.2659\n",
      "Epoch: 104, Average Loss: 0.2645\n",
      "Epoch: 105, Average Loss: 0.2643\n",
      "Epoch: 106, Average Loss: 0.2640\n",
      "Epoch: 107, Average Loss: 0.2642\n",
      "Epoch: 108, Average Loss: 0.2643\n",
      "Epoch: 109, Average Loss: 0.2642\n",
      "Epoch: 110, Average Loss: 0.2643\n",
      "Epoch: 111, Average Loss: 0.2640\n",
      "Epoch: 112, Average Loss: 0.2645\n",
      "Epoch: 113, Average Loss: 0.2649\n",
      "Epoch: 114, Average Loss: 0.2643\n",
      "Epoch: 115, Average Loss: 0.2641\n",
      "Epoch: 116, Average Loss: 0.2639\n",
      "Epoch: 117, Average Loss: 0.2647\n",
      "Epoch: 118, Average Loss: 0.2648\n",
      "Epoch: 119, Average Loss: 0.2652\n",
      "Epoch: 120, Average Loss: 0.2655\n",
      "Epoch: 121, Average Loss: 0.2639\n",
      "Epoch: 122, Average Loss: 0.2648\n",
      "Epoch: 123, Average Loss: 0.2651\n",
      "Epoch: 124, Average Loss: 0.2645\n",
      "Epoch: 125, Average Loss: 0.2640\n",
      "Epoch: 126, Average Loss: 0.2656\n",
      "Epoch: 127, Average Loss: 0.2649\n",
      "Epoch: 128, Average Loss: 0.2642\n",
      "Epoch: 129, Average Loss: 0.2647\n",
      "Epoch: 130, Average Loss: 0.2663\n",
      "Epoch: 131, Average Loss: 0.2645\n",
      "Epoch: 132, Average Loss: 0.2653\n",
      "Epoch: 133, Average Loss: 0.2645\n",
      "Epoch: 134, Average Loss: 0.2640\n",
      "Epoch: 135, Average Loss: 0.2640\n",
      "Epoch: 136, Average Loss: 0.2658\n",
      "Epoch: 137, Average Loss: 0.2641\n",
      "Epoch: 138, Average Loss: 0.2651\n",
      "Epoch: 139, Average Loss: 0.2652\n",
      "Epoch: 140, Average Loss: 0.2656\n",
      "Epoch: 141, Average Loss: 0.2641\n",
      "Epoch: 142, Average Loss: 0.2651\n",
      "Epoch: 143, Average Loss: 0.2649\n",
      "Epoch: 144, Average Loss: 0.2653\n",
      "Epoch: 145, Average Loss: 0.2642\n",
      "Epoch: 146, Average Loss: 0.2643\n",
      "Epoch: 147, Average Loss: 0.2642\n",
      "Epoch: 148, Average Loss: 0.2644\n",
      "Epoch: 149, Average Loss: 0.2642\n",
      "Epoch: 150, Average Loss: 0.2653\n",
      "Epoch: 151, Average Loss: 0.2641\n",
      "Epoch: 152, Average Loss: 0.2644\n",
      "Epoch: 153, Average Loss: 0.2643\n",
      "Epoch: 154, Average Loss: 0.2642\n",
      "Epoch: 155, Average Loss: 0.2645\n",
      "Epoch: 156, Average Loss: 0.2645\n",
      "Epoch: 157, Average Loss: 0.2651\n",
      "Epoch: 158, Average Loss: 0.2645\n",
      "Epoch: 159, Average Loss: 0.2646\n",
      "Epoch: 160, Average Loss: 0.2652\n",
      "Epoch: 161, Average Loss: 0.2639\n",
      "Epoch: 162, Average Loss: 0.2645\n",
      "Epoch: 163, Average Loss: 0.2661\n",
      "Epoch: 164, Average Loss: 0.2645\n",
      "Epoch: 165, Average Loss: 0.2648\n",
      "Epoch: 166, Average Loss: 0.2643\n",
      "Epoch: 167, Average Loss: 0.2643\n",
      "Epoch: 168, Average Loss: 0.2654\n",
      "Epoch: 169, Average Loss: 0.2646\n",
      "Epoch: 170, Average Loss: 0.2653\n",
      "Epoch: 171, Average Loss: 0.2642\n",
      "Epoch: 172, Average Loss: 0.2652\n",
      "Epoch: 173, Average Loss: 0.2640\n",
      "Epoch: 174, Average Loss: 0.2641\n",
      "Epoch: 175, Average Loss: 0.2649\n",
      "Epoch: 176, Average Loss: 0.2641\n",
      "Epoch: 177, Average Loss: 0.2645\n",
      "Epoch: 178, Average Loss: 0.2645\n",
      "Epoch: 179, Average Loss: 0.2662\n",
      "Epoch: 180, Average Loss: 0.2643\n",
      "Epoch: 181, Average Loss: 0.2653\n",
      "Epoch: 182, Average Loss: 0.2643\n",
      "Epoch: 183, Average Loss: 0.2646\n",
      "Epoch: 184, Average Loss: 0.2644\n",
      "Epoch: 185, Average Loss: 0.2641\n",
      "Epoch: 186, Average Loss: 0.2662\n",
      "Epoch: 187, Average Loss: 0.2652\n",
      "Epoch: 188, Average Loss: 0.2642\n",
      "Epoch: 189, Average Loss: 0.2642\n",
      "Epoch: 190, Average Loss: 0.2645\n",
      "Epoch: 191, Average Loss: 0.2642\n",
      "Epoch: 192, Average Loss: 0.2642\n",
      "Epoch: 193, Average Loss: 0.2641\n",
      "Epoch: 194, Average Loss: 0.2640\n",
      "Epoch: 195, Average Loss: 0.2638\n",
      "Epoch: 196, Average Loss: 0.2639\n",
      "Epoch: 197, Average Loss: 0.2642\n",
      "Epoch: 198, Average Loss: 0.2640\n",
      "Epoch: 199, Average Loss: 0.2639\n",
      "Epoch: 200, Average Loss: 0.2653\n",
      "Epoch: 201, Average Loss: 0.2636\n",
      "Epoch: 202, Average Loss: 0.2647\n",
      "Epoch: 203, Average Loss: 0.2643\n",
      "Epoch: 204, Average Loss: 0.2648\n",
      "Epoch: 205, Average Loss: 0.2638\n",
      "Epoch: 206, Average Loss: 0.2636\n",
      "Epoch: 207, Average Loss: 0.2647\n",
      "Epoch: 208, Average Loss: 0.2646\n",
      "Epoch: 209, Average Loss: 0.2642\n",
      "Epoch: 210, Average Loss: 0.2643\n",
      "Epoch: 211, Average Loss: 0.2657\n",
      "Epoch: 212, Average Loss: 0.2650\n",
      "Epoch: 213, Average Loss: 0.2654\n",
      "Epoch: 214, Average Loss: 0.2636\n",
      "Epoch: 215, Average Loss: 0.2650\n",
      "Epoch: 216, Average Loss: 0.2642\n",
      "Epoch: 217, Average Loss: 0.2643\n",
      "Epoch: 218, Average Loss: 0.2640\n",
      "Epoch: 219, Average Loss: 0.2652\n",
      "Epoch: 220, Average Loss: 0.2647\n",
      "Epoch: 221, Average Loss: 0.2648\n",
      "Epoch: 222, Average Loss: 0.2651\n",
      "Epoch: 223, Average Loss: 0.2642\n",
      "Epoch: 224, Average Loss: 0.2658\n",
      "Epoch: 225, Average Loss: 0.2646\n",
      "Epoch: 226, Average Loss: 0.2654\n",
      "Epoch: 227, Average Loss: 0.2651\n",
      "Epoch: 228, Average Loss: 0.2651\n",
      "Epoch: 229, Average Loss: 0.2646\n",
      "Epoch: 230, Average Loss: 0.2647\n",
      "Epoch: 231, Average Loss: 0.2644\n",
      "Epoch: 232, Average Loss: 0.2639\n",
      "Epoch: 233, Average Loss: 0.2647\n",
      "Epoch: 234, Average Loss: 0.2651\n",
      "Epoch: 235, Average Loss: 0.2644\n",
      "Epoch: 236, Average Loss: 0.2638\n",
      "Epoch: 237, Average Loss: 0.2638\n",
      "Epoch: 238, Average Loss: 0.2653\n",
      "Epoch: 239, Average Loss: 0.2637\n",
      "Epoch: 240, Average Loss: 0.2649\n",
      "Epoch: 241, Average Loss: 0.2649\n",
      "Epoch: 242, Average Loss: 0.2653\n",
      "Epoch: 243, Average Loss: 0.2644\n",
      "Epoch: 244, Average Loss: 0.2655\n",
      "Epoch: 245, Average Loss: 0.2644\n",
      "Epoch: 246, Average Loss: 0.2657\n",
      "Epoch: 247, Average Loss: 0.2658\n",
      "Epoch: 248, Average Loss: 0.2652\n",
      "Epoch: 249, Average Loss: 0.2652\n",
      "Epoch: 250, Average Loss: 0.2662\n",
      "Epoch: 251, Average Loss: 0.2673\n",
      "Epoch: 252, Average Loss: 0.2651\n",
      "Epoch: 253, Average Loss: 0.2641\n",
      "Epoch: 254, Average Loss: 0.2652\n",
      "Epoch: 255, Average Loss: 0.2648\n",
      "Epoch: 256, Average Loss: 0.2646\n",
      "Epoch: 257, Average Loss: 0.2649\n",
      "Epoch: 258, Average Loss: 0.2645\n",
      "Epoch: 259, Average Loss: 0.2645\n",
      "Epoch: 260, Average Loss: 0.2651\n",
      "Epoch: 261, Average Loss: 0.2646\n",
      "Epoch: 262, Average Loss: 0.2645\n",
      "Epoch: 263, Average Loss: 0.2644\n",
      "Epoch: 264, Average Loss: 0.2647\n",
      "Epoch: 265, Average Loss: 0.2638\n",
      "Epoch: 266, Average Loss: 0.2638\n",
      "Epoch: 267, Average Loss: 0.2646\n",
      "Epoch: 268, Average Loss: 0.2652\n",
      "Epoch: 269, Average Loss: 0.2642\n",
      "Epoch: 270, Average Loss: 0.2646\n",
      "Epoch: 271, Average Loss: 0.2645\n",
      "Epoch: 272, Average Loss: 0.2641\n",
      "Epoch: 273, Average Loss: 0.2642\n",
      "Epoch: 274, Average Loss: 0.2647\n",
      "Epoch: 275, Average Loss: 0.2647\n",
      "Epoch: 276, Average Loss: 0.2650\n",
      "Epoch: 277, Average Loss: 0.2644\n",
      "Epoch: 278, Average Loss: 0.2645\n",
      "Epoch: 279, Average Loss: 0.2641\n",
      "Epoch: 280, Average Loss: 0.2662\n",
      "Epoch: 281, Average Loss: 0.2642\n",
      "Epoch: 282, Average Loss: 0.2652\n",
      "Epoch: 283, Average Loss: 0.2641\n",
      "Epoch: 284, Average Loss: 0.2646\n",
      "Epoch: 285, Average Loss: 0.2648\n",
      "Epoch: 286, Average Loss: 0.2649\n",
      "Epoch: 287, Average Loss: 0.2645\n",
      "Epoch: 288, Average Loss: 0.2658\n",
      "Epoch: 289, Average Loss: 0.2651\n",
      "Epoch: 290, Average Loss: 0.2648\n",
      "Epoch: 291, Average Loss: 0.2657\n",
      "Epoch: 292, Average Loss: 0.2661\n",
      "Epoch: 293, Average Loss: 0.2651\n",
      "Epoch: 294, Average Loss: 0.2662\n",
      "Epoch: 295, Average Loss: 0.2642\n",
      "Epoch: 296, Average Loss: 0.2656\n",
      "Epoch: 297, Average Loss: 0.2650\n",
      "Epoch: 298, Average Loss: 0.2650\n",
      "Epoch: 299, Average Loss: 0.2651\n",
      "Epoch: 300, Average Loss: 0.2644\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode. This is \n",
    "    # important because certain layers, such \n",
    "    # as dropout or batch normalization, behave\n",
    "    # differently during training than during \n",
    "    # evaluation. model.train() ensures that \n",
    "    # these layers are in training mode.\n",
    "    model.train()\n",
    "    total_loss = 0 # Initialize total loss for the epoch\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Resets the gradients of all model parameters\n",
    "        # to zero before starting the backpropagation\n",
    "        # process for the current batch. This is \n",
    "        # important because gradients are accumulated\n",
    "        # by default in PyTorch, so they need to be\n",
    "        # cleared out before calculating the gradients\n",
    "        # for the current batch\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_x)\n",
    "        # Compute the loss between the model's \n",
    "        # predictions and the actual target values \n",
    "        # (batch_y).\n",
    "        loss = criterion(predictions ,batch_y)\n",
    "        # Compute the gradients of the loss with \n",
    "        # respect to each model parameter using \n",
    "        # backpropagation. These gradients are \n",
    "        # used to update the model parameters \n",
    "        # in the next step.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() # Accumulate the loss for each batch\n",
    "\n",
    "    average_loss = total_loss / len(train_loader) # Compute the average loss for the epoch    \n",
    "    print(f'Epoch: {epoch+1}, Average Loss: {average_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "60fa0214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2030\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "# Switch to Evaluation Mode. In this mode, \n",
    "# certain layers like dropout and batch normalization, \n",
    "# which behave differently during training, will \n",
    "# operate in evaluation mode, meaning they won't \n",
    "# apply dropout or update running statistics.\n",
    "# Why Use eval()?: This ensures that the \n",
    "# model's behavior is consistent during \n",
    "# testing and that the evaluation reflects\n",
    "# the true performance on unseen data.\n",
    "model.eval()\n",
    "\n",
    "# Disabling Gradient Calculation.\n",
    "# The torch.no_grad() context manager \n",
    "# temporarily disables gradient computation. \n",
    "# Since gradients are only necessary during \n",
    "# training (when you need to update the \n",
    "# model's parameters), disabling them \n",
    "# during evaluation saves memory and \n",
    "# computational resources because\n",
    "# pytorch will not track the operations\n",
    "# for that it might need later for gradient\n",
    "# computation.\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(x_test_tensor)\n",
    "    test_loss = criterion(\n",
    "        test_predictions, y_test_tensor \n",
    "    )\n",
    "    print(f'Test loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cc2f3e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.9365, -0.1774,  0.1881],\n",
       "         [ 0.0193,  0.0223,  0.9878],\n",
       "         [-0.1601,  0.4076,  0.7332],\n",
       "         [ 1.1161, -0.2728,  0.0964],\n",
       "         [-0.1235,  0.4665,  0.6038],\n",
       "         [ 0.1379,  0.2769,  0.5351],\n",
       "         [-0.0412,  0.4899,  0.5216],\n",
       "         [ 0.8188,  0.2248, -0.1306],\n",
       "         [ 1.0081, -0.2252,  0.1807],\n",
       "         [ 0.1610,  0.4317,  0.3657],\n",
       "         [ 0.0424,  0.6347,  0.2277],\n",
       "         [-0.4904,  0.8226,  0.6318],\n",
       "         [ 1.0204, -0.1903,  0.1222],\n",
       "         [-0.0812,  0.0865,  0.9926],\n",
       "         [ 1.1653, -0.3515,  0.1499],\n",
       "         [-0.0961,  0.7515,  0.2733],\n",
       "         [-0.1032,  0.1685,  0.9452],\n",
       "         [ 0.8758,  0.0150,  0.0552],\n",
       "         [ 0.9022, -0.1385,  0.2090],\n",
       "         [ 0.8085,  0.2206, -0.1183],\n",
       "         [ 0.9721,  0.0728, -0.0988],\n",
       "         [ 0.9004,  0.1059, -0.0782],\n",
       "         [-0.0260, -0.0853,  1.1556],\n",
       "         [ 0.7987,  0.3128, -0.1947],\n",
       "         [ 0.8597,  0.1792, -0.1143],\n",
       "         [ 0.8798,  0.0994, -0.0600],\n",
       "         [ 0.9514,  0.0287, -0.0445],\n",
       "         [ 0.9310,  0.0579, -0.0560],\n",
       "         [ 0.0917,  0.5919,  0.2515],\n",
       "         [ 0.9697, -0.0460,  0.0262]]),\n",
       " tensor([[ 0.8855, -0.0455, -0.1675],\n",
       "         [-0.1052, -0.0270,  1.1129],\n",
       "         [ 0.1716, -0.1916,  0.9313],\n",
       "         [ 1.0803, -0.1392,  0.0928],\n",
       "         [-0.0317,  0.1776,  0.8104],\n",
       "         [-0.1918,  0.8232,  0.0087],\n",
       "         [ 0.1676,  0.0446,  0.9015],\n",
       "         [ 0.9811, -0.1024, -0.1177],\n",
       "         [ 0.9822,  0.0201,  0.0298],\n",
       "         [ 0.1540,  0.8558, -0.0869],\n",
       "         [ 0.1226,  1.0584, -0.0450],\n",
       "         [ 0.0336, -0.1823,  1.0747],\n",
       "         [ 0.9260, -0.1884, -0.0635],\n",
       "         [ 0.0806,  0.0255,  1.0648],\n",
       "         [ 1.0068,  0.0948,  0.0046],\n",
       "         [-0.0327,  0.9703,  0.0221],\n",
       "         [-0.1317, -0.1811,  0.8136],\n",
       "         [ 0.8086, -0.1583, -0.1137],\n",
       "         [ 1.0176, -0.0975, -0.1991],\n",
       "         [ 0.9081, -0.1197,  0.0826],\n",
       "         [ 1.1356,  0.0016,  0.1326],\n",
       "         [ 1.1033, -0.1465, -0.0207],\n",
       "         [-0.1655, -0.1062,  1.0112],\n",
       "         [ 0.8783,  0.0053,  0.1495],\n",
       "         [ 1.1910, -0.1530, -0.1532],\n",
       "         [ 0.9535, -0.1663, -0.0588],\n",
       "         [ 1.0617, -0.1347,  0.0346],\n",
       "         [ 1.1714, -0.0156, -0.1323],\n",
       "         [ 0.1076,  0.8990,  0.1439],\n",
       "         [ 0.8543, -0.1744,  0.1129]])]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[test_predictions, y_test_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "decb4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1.weight\n",
      "Values:\n",
      "tensor([[ 0.0574,  0.0112,  0.0738, -0.1397],\n",
      "        [-0.3066, -0.1162, -0.2040,  0.4437],\n",
      "        [ 0.1776,  0.0868,  0.2685, -0.0862],\n",
      "        [-0.3237, -0.1017,  0.2918,  0.0324],\n",
      "        [ 0.0139, -0.0123, -0.0874,  0.0684],\n",
      "        [-0.2646,  0.0767,  0.3397, -0.2831],\n",
      "        [ 0.1552,  0.0249,  0.0565, -0.2315],\n",
      "        [ 0.0647,  0.4042, -0.0141,  0.2987],\n",
      "        [ 0.1031, -0.0268,  0.0006, -0.1075],\n",
      "        [ 0.2562,  0.2837,  0.2302,  0.2602],\n",
      "        [-0.0154, -0.0035, -0.0815,  0.1005],\n",
      "        [-0.0462, -0.3333, -0.1279, -0.4275],\n",
      "        [-0.1265,  0.1532,  0.0125, -0.0129],\n",
      "        [-0.2139, -0.0824,  0.1721,  0.0264],\n",
      "        [-0.3658,  0.2446,  0.0938,  0.2989],\n",
      "        [-0.4175, -0.2673,  0.4326, -0.0624],\n",
      "        [-0.0937, -0.0523,  0.4059, -0.3648],\n",
      "        [ 0.1386, -0.0597, -0.2899,  0.2596],\n",
      "        [ 0.2847,  0.1455, -0.2888,  0.1852],\n",
      "        [ 0.0704, -0.0315, -0.1535,  0.0898],\n",
      "        [ 0.2091, -0.1407, -0.3099,  0.0677],\n",
      "        [-0.2651, -0.0158,  0.3605,  0.0298],\n",
      "        [ 0.1072, -0.0280, -0.0605, -0.0475],\n",
      "        [-0.0054,  0.0115,  0.0166, -0.0109],\n",
      "        [-0.0384,  0.0123,  0.0492, -0.0114],\n",
      "        [-0.3126, -0.2472,  0.1654,  0.2963],\n",
      "        [ 0.1188,  0.3288, -0.3257,  0.3030],\n",
      "        [-0.0310,  0.2077, -0.1952,  0.3044],\n",
      "        [ 0.1275, -0.2054,  0.1182, -0.3014],\n",
      "        [ 0.0476,  0.0310,  0.0635, -0.1031],\n",
      "        [ 0.1007, -0.1114,  0.3637, -0.1462],\n",
      "        [-0.0103,  0.0040,  0.0191, -0.0143],\n",
      "        [ 0.2898, -0.1045, -0.1318, -0.1987],\n",
      "        [ 0.1292, -0.0381, -0.0853, -0.0136],\n",
      "        [-0.0186, -0.2714,  0.0960, -0.2816],\n",
      "        [ 0.1385,  0.0711,  0.3674, -0.4309],\n",
      "        [-0.2745,  0.1794, -0.4673,  0.5214],\n",
      "        [ 0.2754,  0.3155,  0.2614,  0.0368],\n",
      "        [ 0.2470, -0.1695, -0.0198, -0.4283],\n",
      "        [ 0.2587, -0.3431,  0.2291, -0.3138],\n",
      "        [ 0.1584,  0.3229, -0.3316,  0.0449],\n",
      "        [-0.2834,  0.2237, -0.4146, -0.1448],\n",
      "        [ 0.1473, -0.0575,  0.0301, -0.0108],\n",
      "        [ 0.1918,  0.0146, -0.2583,  0.1127],\n",
      "        [-0.0878,  0.0533,  0.0467,  0.0541],\n",
      "        [-0.0114, -0.1342,  0.1551, -0.2295],\n",
      "        [-0.0479,  0.2474, -0.1172,  0.1237],\n",
      "        [ 0.1968, -0.0894,  0.1578,  0.1975],\n",
      "        [ 0.2637, -0.2700, -0.1779, -0.1253],\n",
      "        [-0.2606,  0.0301,  0.1864,  0.0401],\n",
      "        [-0.0399, -0.0645, -0.3541, -0.2929],\n",
      "        [ 0.2865, -0.3018,  0.2655, -0.5270],\n",
      "        [ 0.4102, -0.4376, -0.0590, -0.4641],\n",
      "        [ 0.1873,  0.1267,  0.2655,  0.4027],\n",
      "        [-0.0639, -0.0461, -0.1799,  0.1959],\n",
      "        [ 0.4848, -0.0457, -0.2005, -0.3772],\n",
      "        [-0.2042,  0.0289, -0.2176,  0.3609],\n",
      "        [-0.0048, -0.0129,  0.3978, -0.4383],\n",
      "        [-0.0807,  0.2759,  0.1232,  0.0528],\n",
      "        [-0.2774, -0.1367,  0.0249,  0.1100],\n",
      "        [-0.1353,  0.1141,  0.0098,  0.1198],\n",
      "        [-0.1288, -0.0058,  0.2395, -0.1842],\n",
      "        [-0.0210,  0.1174, -0.1479,  0.1245],\n",
      "        [-0.0894, -0.4094,  0.3842, -0.2340],\n",
      "        [ 0.0192, -0.1014,  0.0280, -0.1379],\n",
      "        [-0.2174,  0.1121, -0.1347,  0.3138],\n",
      "        [ 0.3388, -0.1543, -0.0530, -0.2592],\n",
      "        [ 0.1158,  0.0385, -0.1042,  0.0133],\n",
      "        [ 0.2535,  0.1127, -0.0884, -0.1776],\n",
      "        [ 0.3304,  0.2651, -0.2673, -0.1080],\n",
      "        [ 0.0572, -0.0120, -0.3229,  0.2697],\n",
      "        [-0.3277, -0.0347,  0.5814, -0.3355],\n",
      "        [-0.1767, -0.1245,  0.3641, -0.0890],\n",
      "        [ 0.0978,  0.2149, -0.2275, -0.0048],\n",
      "        [-0.3706,  0.1522,  0.3765,  0.1968],\n",
      "        [-0.0831,  0.0314,  0.1535,  0.2069],\n",
      "        [-0.3365, -0.0435, -0.2336,  0.4407],\n",
      "        [ 0.3560, -0.2126, -0.2967, -0.0617],\n",
      "        [ 0.0720, -0.0329, -0.1730,  0.1067],\n",
      "        [ 0.3243, -0.1289,  0.1702, -0.4229],\n",
      "        [ 0.2778, -0.2835, -0.3675, -0.1664],\n",
      "        [ 0.1132,  0.0810,  0.1070, -0.2105],\n",
      "        [-0.0010,  0.0022,  0.0076, -0.0101],\n",
      "        [ 0.1493, -0.0503, -0.1475, -0.0067],\n",
      "        [ 0.0052, -0.0006, -0.0253,  0.0191],\n",
      "        [ 0.2449,  0.0091, -0.0032, -0.0425],\n",
      "        [-0.2583, -0.2649,  0.4351, -0.3713],\n",
      "        [ 0.0197, -0.1390, -0.3419,  0.2303],\n",
      "        [ 0.2604, -0.1235, -0.4470,  0.1704],\n",
      "        [-0.2413,  0.1242,  0.4768, -0.2807],\n",
      "        [ 0.4569,  0.0612, -0.4312, -0.2997],\n",
      "        [-0.1376,  0.0816,  0.3443, -0.2114],\n",
      "        [ 0.4570,  0.3867, -0.4327, -0.1197],\n",
      "        [-0.1899,  0.0526,  0.3622, -0.1675],\n",
      "        [-0.1043,  0.0406,  0.1115, -0.0131],\n",
      "        [-0.0903,  0.2066,  0.0914,  0.0938],\n",
      "        [-0.2683, -0.1407,  0.5007, -0.3620],\n",
      "        [ 0.1182, -0.0392, -0.1072, -0.0132],\n",
      "        [ 0.0497, -0.0211, -0.2088,  0.1616],\n",
      "        [ 0.0495,  0.2522, -0.3419,  0.3294],\n",
      "        [-0.3064,  0.2199,  0.1995,  0.0739],\n",
      "        [-0.4013, -0.2035, -0.0931,  0.4237],\n",
      "        [ 0.1812,  0.0188, -0.0448, -0.1112],\n",
      "        [-0.2268,  0.2726, -0.2097,  0.2830],\n",
      "        [-0.0764, -0.0091,  0.3523, -0.3581],\n",
      "        [ 0.0711, -0.0199,  0.0673, -0.1372],\n",
      "        [-0.2648,  0.1486,  0.3319, -0.0329],\n",
      "        [-0.0451,  0.0229,  0.1240, -0.0780],\n",
      "        [ 0.1988, -0.0067, -0.1792, -0.3322],\n",
      "        [ 0.0166,  0.1124, -0.1677,  0.2381],\n",
      "        [ 0.3458,  0.2132, -0.3595, -0.1104],\n",
      "        [-0.1392, -0.0737, -0.0750,  0.2332],\n",
      "        [ 0.3429, -0.0565, -0.1749, -0.4133],\n",
      "        [ 0.0756, -0.2585, -0.2779,  0.1272],\n",
      "        [ 0.2457, -0.2707,  0.2082, -0.4237],\n",
      "        [ 0.1946, -0.0708, -0.1851, -0.0037],\n",
      "        [ 0.3033, -0.2390,  0.1564, -0.3726],\n",
      "        [ 0.3707, -0.3270,  0.2963,  0.2484],\n",
      "        [ 0.1666,  0.1536,  0.3197, -0.3670],\n",
      "        [-0.4660,  0.1246,  0.3358,  0.2314],\n",
      "        [ 0.2830,  0.0153, -0.3031, -0.1236],\n",
      "        [-0.2478, -0.2897, -0.2825,  0.1082],\n",
      "        [-0.0396,  0.1430,  0.0408,  0.1633],\n",
      "        [ 0.1093, -0.0169, -0.0580, -0.0562],\n",
      "        [-0.2102,  0.0681,  0.2648, -0.0577],\n",
      "        [ 0.0629, -0.0345, -0.1929,  0.1236],\n",
      "        [ 0.2460,  0.1156, -0.2399,  0.2681],\n",
      "        [-0.2567,  0.1084,  0.4156, -0.1704]])\n",
      "\n",
      "Layer: fc1.bias\n",
      "Values:\n",
      "tensor([ 4.4299e-02, -1.6719e-01,  2.6441e-01, -1.2447e-01, -1.2334e-03,\n",
      "         2.5943e-01,  7.5872e-02,  1.9439e-01,  3.5211e-03,  8.7797e-02,\n",
      "         1.4679e-03, -3.6084e-01,  1.1714e-01, -1.9151e-01, -8.1628e-02,\n",
      "         3.3040e-02, -6.8953e-02,  1.6514e-01, -1.6897e-01, -2.1783e-03,\n",
      "         4.1740e-02, -2.3977e-01, -1.3124e-03, -1.3928e-02,  1.2365e-03,\n",
      "        -9.0466e-02, -2.1107e-01,  1.5972e-01,  5.9330e-03,  2.7890e-02,\n",
      "         3.0846e-01,  6.4567e-04, -1.7829e-01,  7.5121e-02,  2.4298e-02,\n",
      "         3.1899e-01,  1.3698e-01, -3.4507e-01, -1.2025e-01, -3.7582e-01,\n",
      "         2.7022e-01,  4.1412e-01, -6.3319e-02, -3.2554e-02,  3.7212e-02,\n",
      "         3.6247e-02,  5.9010e-02,  3.6695e-01,  1.3326e-01,  6.9890e-02,\n",
      "         2.8139e-02,  3.4666e-01,  3.4439e-01,  2.1568e-01, -1.1382e-01,\n",
      "        -1.6083e-01, -7.9420e-02,  2.6363e-01,  6.4258e-02,  4.3720e-02,\n",
      "        -2.0771e-01,  1.4991e-01, -2.1524e-01,  3.0909e-01, -5.2075e-03,\n",
      "         1.1729e-01, -5.2045e-02, -4.6873e-02,  4.0189e-03, -3.1279e-01,\n",
      "        -5.9374e-02,  3.3726e-02,  1.4015e-02,  4.5072e-02, -3.0550e-01,\n",
      "         1.4238e-01, -3.0031e-01, -1.0352e-01, -1.4287e-03,  2.6420e-01,\n",
      "         1.2735e-01,  2.9817e-02, -8.3690e-04, -5.8127e-05,  1.3740e-02,\n",
      "        -6.4410e-03,  2.2518e-01,  9.4997e-02, -4.9485e-02, -4.2223e-02,\n",
      "        -2.9532e-01,  8.1306e-03,  3.0920e-01,  6.6726e-02,  3.0430e-05,\n",
      "        -1.4272e-01,  3.0857e-01, -9.2797e-04,  2.8824e-02,  3.8236e-01,\n",
      "         6.5557e-02,  3.1552e-01,  4.6482e-02,  3.0445e-02, -1.8023e-01,\n",
      "         8.7655e-03, -4.8919e-02,  1.8309e-03, -1.5965e-01, -4.9291e-02,\n",
      "        -8.5402e-02,  5.9021e-02,  1.5987e-01,  1.0072e-01, -2.4011e-01,\n",
      "         4.8913e-03,  3.6973e-01,  1.4925e-01, -1.2123e-01,  2.3180e-01,\n",
      "        -1.7362e-01,  2.5425e-02, -1.4061e-01, -1.1682e-01,  1.1291e-03,\n",
      "        -3.2152e-03, -2.0598e-01, -6.2729e-03])\n",
      "\n",
      "Layer: fc2.weight\n",
      "Values:\n",
      "tensor([[-1.9884e-02,  7.9323e-04, -7.3943e-02,  ..., -3.0325e-02,\n",
      "          5.4309e-02,  5.5899e-02],\n",
      "        [-3.5862e-02, -3.1339e-02,  1.4767e-02,  ...,  1.0842e-02,\n",
      "         -6.5552e-03,  7.7780e-02],\n",
      "        [ 2.8634e-02,  3.6986e-02,  1.9582e-02,  ..., -5.3951e-02,\n",
      "         -4.0739e-02, -9.1493e-05],\n",
      "        ...,\n",
      "        [-6.0823e-02, -4.7700e-02,  3.9005e-02,  ...,  8.2596e-02,\n",
      "          4.7956e-03, -3.4246e-02],\n",
      "        [-8.6139e-02,  6.4813e-02,  8.4794e-02,  ...,  4.2530e-02,\n",
      "         -3.0043e-02, -3.9794e-02],\n",
      "        [-1.1329e-02, -6.9920e-02,  5.2028e-02,  ...,  5.7178e-02,\n",
      "          8.1273e-02, -4.1342e-02]])\n",
      "\n",
      "Layer: fc2.bias\n",
      "Values:\n",
      "tensor([ 0.0030, -0.0083, -0.0210,  0.0084,  0.0295,  0.0479,  0.0092, -0.0229,\n",
      "        -0.0217,  0.0034, -0.0583,  0.0095,  0.0239,  0.0405,  0.0998,  0.0144,\n",
      "         0.0917,  0.0361,  0.0070,  0.0110, -0.0091,  0.0409,  0.0290, -0.0560,\n",
      "         0.0498,  0.0369, -0.0127,  0.0792, -0.0594,  0.0723,  0.0035, -0.0470,\n",
      "         0.0435,  0.0337, -0.0875, -0.0265, -0.0768,  0.0238,  0.0375, -0.0205,\n",
      "        -0.0199,  0.0347, -0.0449,  0.0299,  0.0370, -0.0543,  0.0401,  0.0377,\n",
      "        -0.0637,  0.0122,  0.0248, -0.0314, -0.0211,  0.0374,  0.0382,  0.0706,\n",
      "        -0.0491, -0.0138, -0.0804,  0.0579,  0.0161,  0.0371,  0.0226,  0.0060,\n",
      "        -0.0196, -0.0193,  0.0881,  0.0703, -0.0090,  0.0372,  0.0248, -0.0297,\n",
      "         0.0932, -0.0177, -0.0272,  0.0332,  0.0534,  0.0042, -0.0885, -0.0395,\n",
      "        -0.0360,  0.0063,  0.0432,  0.0532, -0.0441, -0.0665,  0.0454,  0.0646,\n",
      "         0.0290, -0.0292, -0.0586, -0.0480,  0.0761, -0.0428, -0.0418,  0.0010,\n",
      "         0.0619, -0.0384, -0.0298, -0.0412, -0.0207, -0.0374, -0.0857, -0.0083,\n",
      "        -0.0486, -0.0748,  0.0368, -0.0860,  0.0472, -0.0867,  0.0125, -0.0406,\n",
      "        -0.0470, -0.0253,  0.0151, -0.0225,  0.0642, -0.0363, -0.0351, -0.0633,\n",
      "         0.0596, -0.0428,  0.0281,  0.0424,  0.0306, -0.0327,  0.0478, -0.0337,\n",
      "         0.0480, -0.0617,  0.0049, -0.0143, -0.1009,  0.0199,  0.0619,  0.0134,\n",
      "         0.0646, -0.0251, -0.0606,  0.0450,  0.0684, -0.0690, -0.0173, -0.0177,\n",
      "         0.0314,  0.0690, -0.0646, -0.0120, -0.0108,  0.0276,  0.0564,  0.0877,\n",
      "        -0.0283, -0.0537,  0.0736,  0.1003, -0.0546, -0.0624,  0.0071, -0.0112,\n",
      "        -0.0794, -0.0714,  0.0130,  0.0411,  0.0213,  0.0806,  0.0925, -0.0476,\n",
      "        -0.0634,  0.0554,  0.0476,  0.0322,  0.0259, -0.0646,  0.0241,  0.0093,\n",
      "        -0.0934,  0.0364,  0.0725,  0.0075,  0.0605,  0.0366, -0.0314, -0.0478,\n",
      "        -0.0221,  0.0488,  0.0121,  0.0355, -0.0431, -0.0327, -0.0543,  0.0585,\n",
      "        -0.0532, -0.0564,  0.0532, -0.0337,  0.0764,  0.0514,  0.0404, -0.0620,\n",
      "         0.0166, -0.0066,  0.0786,  0.0020,  0.0449,  0.0177, -0.0091, -0.0416,\n",
      "         0.0403,  0.0276, -0.0862,  0.0260, -0.0696,  0.0160, -0.0624, -0.0445,\n",
      "        -0.0489,  0.0738, -0.0433, -0.0275,  0.0420,  0.0447, -0.0029, -0.0225,\n",
      "         0.0229, -0.0704,  0.0338,  0.0011,  0.0775,  0.0225, -0.0709, -0.0715,\n",
      "         0.0275,  0.0315,  0.0332,  0.0787,  0.0508, -0.0061,  0.0560,  0.0150,\n",
      "         0.0460,  0.0114, -0.0523,  0.0178, -0.0287, -0.0132, -0.0131, -0.0627,\n",
      "        -0.0377, -0.0468, -0.0388, -0.0241, -0.0620, -0.0577, -0.0226, -0.0018])\n",
      "\n",
      "Layer: fc3.weight\n",
      "Values:\n",
      "tensor([[ 0.0149, -0.0583,  0.0062,  ..., -0.0423,  0.0133, -0.0031],\n",
      "        [ 0.0197, -0.0731,  0.0091,  ..., -0.0050,  0.0538, -0.0442],\n",
      "        [ 0.0116, -0.0160, -0.0317,  ...,  0.0002, -0.0117,  0.0030],\n",
      "        ...,\n",
      "        [ 0.0704, -0.0165,  0.0227,  ...,  0.0317, -0.0307, -0.0138],\n",
      "        [ 0.0286, -0.0035, -0.0032,  ...,  0.0169, -0.0039, -0.0101],\n",
      "        [ 0.0199,  0.0281,  0.0129,  ..., -0.0061,  0.0058, -0.0285]])\n",
      "\n",
      "Layer: fc3.bias\n",
      "Values:\n",
      "tensor([ 1.1926e-02,  2.4986e-02, -1.2869e-02, -1.2454e-01, -1.3279e-02,\n",
      "        -2.3891e-02,  2.1106e-01,  2.6267e-03,  3.2883e-02, -1.8474e-02,\n",
      "        -8.1039e-03,  2.0485e-02, -3.4910e-02,  5.2977e-02,  9.8819e-03,\n",
      "         3.3876e-02,  1.3410e-01,  6.1115e-02,  6.0380e-03, -2.5336e-02,\n",
      "         1.3213e-01,  6.4412e-03,  6.8034e-03, -3.0558e-02,  1.5113e-02,\n",
      "         5.6377e-04,  1.0532e-02,  7.2710e-04, -2.9093e-02,  1.3144e-02,\n",
      "         1.9846e-02,  3.6297e-04,  2.5113e-02,  4.1276e-02, -1.0791e-01,\n",
      "        -6.4368e-04, -2.7025e-02,  1.1444e-02,  2.1719e-02,  1.7599e-02,\n",
      "         1.7792e-02,  1.0341e-02,  9.8920e-02, -3.3710e-02,  9.8696e-03,\n",
      "        -2.8099e-02, -1.2729e-03,  2.1560e-01, -1.4871e-02,  2.2345e-02,\n",
      "        -3.5198e-03, -4.3738e-03,  1.3337e-02, -1.8534e-03,  3.1778e-03,\n",
      "         1.0916e-02, -1.0014e-02, -1.3274e-02,  1.3386e-03,  1.8754e-02,\n",
      "        -4.7181e-03, -7.0637e-04, -3.7718e-03, -7.4028e-03,  2.1223e-02,\n",
      "        -1.0804e-02,  2.0870e-02, -5.3637e-03, -4.7077e-03, -2.5114e-02,\n",
      "         1.9882e-02,  4.0325e-02,  2.4209e-02, -7.2571e-03,  2.0476e-02,\n",
      "         5.2673e-03,  9.7228e-03,  5.5871e-02,  3.8100e-03,  3.2866e-03,\n",
      "         5.3874e-03, -1.3156e-02,  4.8546e-02,  2.6228e-02, -1.7887e-02,\n",
      "         3.0174e-02, -3.0143e-02, -6.6661e-03,  4.9546e-03,  3.5770e-02,\n",
      "         2.8000e-03, -2.9801e-02, -1.8898e-02, -1.3027e-02, -4.8060e-03,\n",
      "         3.8330e-03,  1.1985e-02, -3.9483e-03,  3.7348e-02,  1.0326e-02,\n",
      "        -1.3644e-02,  1.6252e-02, -1.2432e-02, -1.4943e-02,  2.2261e-02,\n",
      "         1.3505e-02,  5.6780e-03,  2.6253e-02, -4.6120e-02,  9.0994e-02,\n",
      "         3.6565e-02,  8.6982e-03,  1.0448e-02, -1.9253e-02, -1.6218e-02,\n",
      "         4.6559e-02,  5.6680e-03,  1.3011e-02, -1.5054e-03, -7.8385e-03,\n",
      "        -1.9815e-02,  2.5019e-02, -5.3166e-03,  3.6445e-02,  4.7355e-02,\n",
      "         1.3539e-02,  1.7133e-02,  2.9810e-03, -2.6640e-02, -5.7470e-04,\n",
      "        -2.5848e-02,  1.6159e-01,  1.2343e-02,  4.3074e-02,  7.0130e-03,\n",
      "         2.6744e-02,  9.0448e-03,  5.6205e-03,  9.9096e-03, -1.2615e-03,\n",
      "         4.6478e-03, -8.1254e-03,  1.5691e-03, -1.3644e-02,  9.1858e-02,\n",
      "        -2.0825e-02, -2.3243e-02, -5.4910e-03, -3.5030e-02,  4.1574e-03,\n",
      "        -8.3208e-02, -7.7669e-03, -3.2088e-03, -1.9644e-02, -1.6182e-02,\n",
      "         7.2900e-03,  5.5149e-03,  3.6140e-03,  1.8037e-02,  5.1922e-04,\n",
      "         2.6713e-02,  2.2840e-02,  1.4485e-02, -1.8212e-02, -1.6986e-02,\n",
      "         1.3353e-03,  1.8823e-02,  1.1286e-02, -1.2805e-02, -1.1671e-02,\n",
      "        -1.7411e-02, -4.3085e-03,  1.8245e-02,  2.8932e-02, -1.2008e-02,\n",
      "        -1.8464e-02, -3.1682e-02,  3.2397e-03,  1.6772e-02, -9.8720e-03,\n",
      "        -2.0335e-02,  8.9431e-02, -1.8129e-02, -1.7877e-02,  1.5957e-02,\n",
      "         1.2802e-02,  9.1734e-05,  2.0797e-02, -4.9233e-02,  1.2495e-02,\n",
      "         1.0340e-02, -1.2402e-01,  1.9175e-02,  2.3323e-02, -1.9954e-02,\n",
      "        -4.0333e-02,  1.6828e-02,  5.8849e-03,  1.3737e-02,  4.6012e-02,\n",
      "         1.2472e-02,  1.7502e-03,  7.1283e-03, -2.2664e-02, -8.2620e-03,\n",
      "        -2.0016e-02,  1.6775e-02,  9.0146e-03, -2.2676e-02,  1.0355e-01,\n",
      "         1.4150e-02,  3.1048e-02,  8.8288e-02, -2.1776e-01,  1.0630e-02,\n",
      "         1.0238e-02, -5.1978e-04,  5.8179e-03,  3.1996e-03, -3.5763e-03,\n",
      "        -1.7031e-03,  3.0267e-03,  3.0953e-02,  5.4788e-02, -2.9361e-03,\n",
      "        -9.4498e-02,  2.7188e-02,  1.1870e-02,  3.8525e-02, -1.7796e-02,\n",
      "         1.3387e-02, -1.3230e-02, -2.2980e-03, -8.7476e-03, -3.4767e-02,\n",
      "         2.1003e-03, -6.7593e-03,  1.2332e-02,  1.8251e-02,  2.9067e-02,\n",
      "         2.5129e-01, -1.7057e-01,  1.8923e-02, -1.1172e-01, -1.3164e-02,\n",
      "         2.1652e-02, -5.7741e-03,  2.4359e-01, -9.9337e-03,  8.2508e-02,\n",
      "         1.3085e-02, -2.2970e-02, -1.5144e-02, -1.7737e-02, -1.3904e-03,\n",
      "        -7.6897e-03,  6.2844e-03, -1.1348e-02, -9.8427e-03, -1.5295e-02,\n",
      "         4.9057e-02, -2.0357e-02, -8.7589e-03, -1.9585e-02, -5.8944e-03,\n",
      "         4.5802e-03, -1.2805e-04, -9.9470e-03, -2.4411e-02,  4.4261e-02,\n",
      "        -5.8867e-03,  1.4653e-04, -2.8639e-02, -9.1366e-03,  1.7198e-02,\n",
      "         1.7449e-01, -1.8606e-02, -1.7092e-02,  9.4985e-03, -5.9237e-03,\n",
      "        -2.1518e-01,  4.8543e-03, -5.7828e-03, -3.1067e-03, -1.4193e-02,\n",
      "         4.3031e-02,  1.1720e-02, -7.1605e-02, -1.0238e-02,  9.4115e-03,\n",
      "         4.9246e-03,  1.5738e-02, -9.9902e-03, -2.0706e-02, -2.0685e-03,\n",
      "        -8.8734e-03, -6.8307e-04,  2.0195e-02, -1.0619e-02, -9.0447e-03,\n",
      "        -1.7176e-02, -1.4567e-02,  1.2397e-02, -7.0172e-02, -2.4329e-02,\n",
      "        -4.6787e-03,  2.5749e-02,  1.2567e-03, -1.2275e-01,  2.5276e-02,\n",
      "        -2.2372e-03, -3.4727e-02, -2.1454e-02,  2.6911e-02,  3.6693e-02,\n",
      "        -1.4074e-01,  7.3315e-03, -2.7386e-03, -6.9859e-04, -1.1676e-03,\n",
      "         2.7063e-02,  6.2262e-03, -1.7278e-02, -1.4483e-02,  2.5323e-02,\n",
      "         8.3525e-03,  1.5339e-02,  2.3082e-01, -2.3553e-02, -1.6330e-02,\n",
      "         2.5620e-01,  2.3207e-02, -2.0965e-02,  3.4821e-02, -3.6905e-02,\n",
      "        -4.2018e-02,  1.0718e-02,  4.0064e-03, -1.1610e-02, -2.8487e-03,\n",
      "         1.8726e-02, -7.8904e-03,  3.0342e-02,  2.6822e-02, -5.1816e-03,\n",
      "         9.3060e-03,  1.4922e-01, -8.6950e-03,  6.1624e-03,  3.9882e-03,\n",
      "        -1.3900e-02,  1.0414e-03,  2.5156e-02, -4.8442e-03, -1.0509e-02,\n",
      "         1.1044e-01,  7.5005e-03, -9.3947e-03,  8.5537e-03,  5.9645e-02,\n",
      "        -2.8273e-02, -4.7482e-03,  1.7886e-02,  3.9466e-02, -1.8514e-02,\n",
      "        -6.1344e-03, -5.3817e-03,  1.1696e-02,  1.6535e-02,  4.1170e-02,\n",
      "        -1.2393e-02, -1.2500e-01,  2.0309e-02, -3.1077e-02, -1.3239e-02,\n",
      "        -3.6334e-02, -1.3157e-01, -1.1499e-02,  4.0443e-03, -1.1143e-02,\n",
      "         3.1341e-02, -1.2205e-03, -9.1960e-03,  3.1762e-03, -1.8645e-02,\n",
      "        -2.0695e-02,  5.9059e-02,  2.3142e-02, -4.8164e-03,  9.8349e-03,\n",
      "        -4.6896e-02, -2.2178e-02,  1.6167e-02, -2.8115e-03, -9.9314e-03,\n",
      "         6.4768e-03, -2.1555e-03,  1.4923e-02, -1.4796e-01,  3.2644e-02,\n",
      "         1.0955e-02, -7.6503e-03,  1.4611e-02, -4.4036e-02, -8.8268e-03,\n",
      "        -1.9611e-02, -1.3043e-01,  2.1122e-01, -1.2086e-02,  1.3986e-02,\n",
      "        -3.3020e-02,  1.8278e-02, -2.3446e-02, -5.4278e-04, -1.0164e-03,\n",
      "        -2.2015e-02, -1.4708e-02, -1.8318e-02, -2.1289e-02,  1.9846e-02,\n",
      "         4.3894e-02,  1.6839e-02,  2.4833e-02,  2.3190e-02, -9.3877e-02,\n",
      "         1.8423e-02,  2.6255e-03, -1.1571e-02,  2.8780e-02, -1.4828e-02,\n",
      "        -2.7251e-02, -1.3677e-02, -3.8169e-03, -1.0079e-02,  7.0402e-03,\n",
      "         7.2387e-03,  1.8456e-02,  1.2444e-02,  1.6178e-02, -2.2098e-02,\n",
      "         1.0320e-02,  1.3783e-02, -1.3968e-02, -2.3788e-02,  1.4052e-02,\n",
      "         2.8896e-03, -2.1832e-01, -5.7758e-04, -9.6311e-03, -1.9331e-02,\n",
      "        -2.3208e-03, -1.0993e-01,  1.5330e-02, -1.5488e-02,  1.7495e-02,\n",
      "        -2.0105e-02,  5.9287e-03, -1.0703e-02,  2.2630e-02,  1.7794e-02,\n",
      "         1.1731e-02, -4.1760e-03,  2.4158e-02, -2.3951e-02,  4.1432e-02,\n",
      "         9.7082e-03,  9.7738e-03,  1.9962e-03, -1.3809e-02,  6.1309e-03,\n",
      "        -2.0686e-02,  1.6121e-02, -2.3157e-02, -1.9617e-03, -1.0362e-02,\n",
      "        -1.8036e-02, -1.9297e-02, -4.9015e-02, -2.0507e-02, -1.1723e-02,\n",
      "         1.0319e-02,  1.8078e-01,  3.6013e-02,  5.9568e-03,  1.3657e-02,\n",
      "         1.4502e-02, -1.4273e-02,  2.3171e-02,  1.9896e-02,  2.2008e-02,\n",
      "         1.4491e-02,  4.1873e-02, -4.0191e-02, -2.4539e-02, -1.6520e-03,\n",
      "        -3.0913e-02, -1.3566e-01, -2.6196e-02, -3.4642e-02, -8.0273e-03,\n",
      "        -7.1025e-03, -1.7027e-03,  1.4505e-02,  2.3494e-02, -9.0126e-03,\n",
      "         2.2989e-02, -2.6987e-02,  1.8448e-03,  3.6401e-02,  3.4373e-03,\n",
      "        -1.2230e-01,  1.0046e-02])\n",
      "\n",
      "Layer: fc4.weight\n",
      "Values:\n",
      "tensor([[ 1.0416e-04,  2.2865e-03,  4.5886e-05,  ..., -6.7007e-04,\n",
      "         -1.9555e-03, -1.4516e-03],\n",
      "        [ 1.0319e-04, -8.5961e-04,  5.0656e-04,  ...,  1.0386e-03,\n",
      "          2.0643e-02,  1.8608e-03],\n",
      "        [ 3.6416e-05,  1.1913e-03, -5.2897e-04,  ..., -5.5632e-04,\n",
      "         -8.0760e-02, -1.8990e-03]])\n",
      "\n",
      "Layer: fc4.bias\n",
      "Values:\n",
      "tensor([0.1312, 0.1105, 0.1373])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_weights_biases(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'Layer: {name}')\n",
    "            print(f'Values:\\n{param.data}\\n')\n",
    "\n",
    "print_weights_biases(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "03c006b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_data = [[6.7, 2.5, 6, 1.2],\n",
    "            [5.4, 3.4, 1.5, 0.4],\n",
    "            ]\n",
    "\n",
    "# Transform data uring scaler\n",
    "my_data_transformed = scaler.transform(my_test_data)\n",
    "\n",
    "# Turn transformed data into tensor\n",
    "my_data_tensor = torch.tensor(\n",
    "    my_data_transformed, dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4bdbe363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1:\n",
      "- Setosa Score: -0.3148\n",
      "- Versicolor Score: 1.1068\n",
      "- Virginia Score: 0.0612\n",
      "Prediction 2:\n",
      "- Setosa Score: 0.8965\n",
      "- Versicolor Score: 0.0592\n",
      "- Virginia Score: 0.0009\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(my_data_tensor)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Prediction {i + 1}:')\n",
    "    print(f'- Setosa Score: {prediction[0]:.4f}')\n",
    "    print(f'- Versicolor Score: {prediction[1]:.4f}')\n",
    "    print(f'- Virginia Score: {prediction[2]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a3f57fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6.7, 2.5, 6, 1.2], [5.4, 3.4, 1.5, 0.4]]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ef3cd3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.03417754, -1.20786457,  1.2632157 , -0.08154786],\n",
       "       [-0.59215004,  0.99195623, -1.44690142, -1.18416683]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_transformed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
